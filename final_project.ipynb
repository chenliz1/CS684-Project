{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 7: self-supervised single-image depth-estimation using neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors:\n",
    "#### ◘ Sharhad Bashar\n",
    "#### ◘ Lizhe Chen\n",
    "#### ◘ Genséric Ghiro\n",
    "#### ◘ Futian Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Abstract: In a few short sentences highlighting the main points of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Introduction (3-4 paragraphs): reviewing your topic, related technical ideas/algorithms, your selected methodology/approach, its motivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Contributions section: This section should have one separate short paragraph (or a bullet) dedicated to each co-author clearly indicating her/his specific contributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharhad Bashar:\n",
    "       •\n",
    "       •\n",
    "       •\n",
    "       \n",
    "#### Lizhe Chen:\n",
    "       •\n",
    "       •\n",
    "       •\n",
    "       \n",
    "#### Genséric Ghiro:\n",
    "       •\n",
    "       •\n",
    "       •\n",
    "       \n",
    "#### Futian Zhang:\n",
    "       •\n",
    "       •\n",
    "       •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Outline section: the overall structure/organization of the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5aa) Imports\n",
    "5a) Dataloader\n",
    "5b) Network\n",
    "5c) Validator\n",
    "5d) Trainer\n",
    "5e) Training\n",
    "6) Results display\n",
    "7) Conclusion\n",
    "8) References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5aa) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image \n",
    "import torch\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#5a)\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as tF\n",
    "\n",
    "#5b)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import torchvision.models as models\n",
    "from loss import MonodepthLoss #make sure loss.py is in folder\n",
    "\n",
    "#5c) \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#5d)\n",
    "import pickle\n",
    "\n",
    "#5e)\n",
    "from torch.utils.data.dataloader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a) Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "        \n",
    "class JointRandomFlip(object):\n",
    "    def __call__(self, L, R):\n",
    "        if np.random.random_sample()>0.5:\n",
    "            return (tF.hflip(R),tF.hflip(L))\n",
    "        return (L,R)\n",
    "    \n",
    "class JointRandomColorAug(object):\n",
    "\n",
    "    def __init__(self,gamma=(0.8,1.2),brightness=(0.5,2.0),color_shift=(0.8,1.2)):\n",
    "        self.gamma = gamma\n",
    "        self.brightness = brightness\n",
    "        self.color_shift = color_shift\n",
    "\n",
    "    def __call__(self, L, R):\n",
    "        if  np.random.random_sample()>0.5:\n",
    "            \n",
    "            random_gamma = np.random.uniform(*self.gamma)\n",
    "            L_aug = L ** random_gamma\n",
    "            R_aug = R ** random_gamma\n",
    "\n",
    "            random_brightness = np.random.uniform(*self.brightness)\n",
    "            L_aug = L_aug * random_brightness\n",
    "            R_aug = R_aug * random_brightness\n",
    "\n",
    "            random_colors = np.random.uniform(self.color_shift[0],self.color_shift[1], 3)\n",
    "            for i in range(3):\n",
    "                L_aug[i, :, :] *= random_colors[i]\n",
    "                R_aug[i, :, :] *= random_colors[i]\n",
    "\n",
    "            # saturate\n",
    "            L_aug = torch.clamp(L_aug, 0, 1)\n",
    "            R_aug = torch.clamp(R_aug, 0, 1)\n",
    "\n",
    "            return L_aug, R_aug\n",
    "\n",
    "        else:\n",
    "            return L, R\n",
    "\n",
    "class JointToTensor(object):\n",
    "    def __call__(self, L, R):\n",
    "        return tF.to_tensor(L),tF.to_tensor(R)\n",
    "    \n",
    "class JointToImage(object):\n",
    "    def __call__(self, L, R):\n",
    "        return transforms.ToPILImage()(L),transforms.ToPILImage()(R)\n",
    "    \n",
    "    \n",
    "class JointCompose(object):\n",
    "    def __init__(self, transforms):\n",
    "        \"\"\"\n",
    "        params: \n",
    "           transforms (list) : list of transforms\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "    # We override the __call__ function such that this class can be\n",
    "    # called as a function i.e. JointCompose(transforms)(img, target)\n",
    "    # Such classes are known as \"functors\"\n",
    "    def __call__(self, img, target):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            img (PIL.Image)    : input image\n",
    "            target (PIL.Image) : ground truth label \n",
    "        \"\"\"\n",
    "        assert img.size == target.size\n",
    "        for t in self.transforms:\n",
    "            img, target = t(img, target)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class TwoViewDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 resize_shape=(512,256), \n",
    "                 is_train=False,\n",
    "                 transforms=None,\n",
    "                 sanity_check=None):\n",
    "        super(TwoViewDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.interp = Image.ANTIALIAS\n",
    "        self.resize_shape = resize_shape\n",
    "        self.is_train = is_train\n",
    "        self.transforms=transforms\n",
    "        self.loader = pil_loader\n",
    "        \n",
    "        if is_train:\n",
    "            self.imgR_folder = os.path.join(data_path, \"train\", \"image_right\")\n",
    "            self.imgL_folder = os.path.join(data_path, \"train\", \"image_left\")\n",
    "        else:\n",
    "            self.imgR_folder = os.path.join(data_path, \"val\", \"image_right\")\n",
    "            self.imgL_folder = os.path.join(data_path, \"val\", \"image_left\")\n",
    "        \n",
    "        \n",
    "        self.imgR=[os.path.join(self.imgR_folder, x) for x in os.listdir(self.imgR_folder)]\n",
    "        self.imgL=[os.path.join(self.imgL_folder, x) for x in os.listdir(self.imgL_folder)]\n",
    "\n",
    "    def get_color(self, path, do_flip):\n",
    "        color = self.loader(path)\n",
    "        if do_flip:\n",
    "            color = color.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return self.to_tensor(color)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(glob.glob1(self.imgL_folder, \"*.jpg\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(np.array(Image.open(self.imgR[index]).convert('RGB')).shape)\n",
    "        colorR=Image.open(self.imgR[index]).convert('RGB').resize(self.resize_shape)\n",
    "        colorL=Image.open(self.imgL[index]).convert('RGB').resize(self.resize_shape)\n",
    "        #print(np.array(colorR).shape)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            colorR, colorL = self.transforms(colorR, colorL)\n",
    "        return colorL, colorR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_disp(nn.Module):\n",
    "    def __init__(self, num_in_channels):\n",
    "        super(get_disp, self).__init__()\n",
    "        self.p2d = (1, 1, 1, 1)\n",
    "        self.disp = nn.Sequential(nn.Conv2d(num_in_channels, 2, kernel_size=3, stride=1),\n",
    "                                  nn.BatchNorm2d(2),\n",
    "                                  torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.disp(F.pad(x, self.p2d))\n",
    "        return 0.3 * x\n",
    "\n",
    "\n",
    "class iconv(nn.Module):\n",
    "    def __init__(self, num_in_channels, num_out_channels, kernel_size, stride):\n",
    "        super(iconv, self).__init__()\n",
    "        p = int(np.floor((kernel_size - 1) / 2))\n",
    "        self.p2d = p2d = (p, p, p, p)\n",
    "\n",
    "        self.iconv = nn.Sequential(nn.Conv2d(num_in_channels, num_out_channels, kernel_size=kernel_size, stride=stride),\n",
    "                                  nn.BatchNorm2d(num_out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.iconv(F.pad(x, self.p2d))\n",
    "        return F.elu(x, inplace=True)\n",
    "\n",
    "class upconv(nn.Module):\n",
    "    def __init__(self, num_in_channels, num_out_channels, kernel_size, scale):\n",
    "        super(upconv, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.conv1 = iconv(num_in_channels, num_out_channels, kernel_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale, mode='bilinear', align_corners=True)\n",
    "        return self.conv1(x)\n",
    "\n",
    "class ResnetDispModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_input_channel, encoder='resnet18', pretrained=True, criterion=None):\n",
    "        super(ResnetDispModel, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.num_input_channel = num_input_channel\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        filters_res18 = [64, 128, 256, 512]\n",
    "        resnet_pool1 = list(resnet.children())[1:4]\n",
    "\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.maxpool = nn.Sequential(*resnet_pool1)\n",
    "\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "\n",
    "        self.upconv6 = upconv(filters_res18[3], 512, 3, 2)\n",
    "        self.iconv6 = iconv(filters_res18[2] + 512, 512, 3, 1)\n",
    "\n",
    "        self.upconv5 = upconv(512, 256, 3, 2)\n",
    "        self.iconv5 = iconv(filters_res18[1] + 256, 256, 3, 1)\n",
    "\n",
    "        self.upconv4 = upconv(256, 128, 3, 2)\n",
    "        self.iconv4 = iconv(filters_res18[0] + 128, 128, 3, 1)\n",
    "        self.disp4_layer = get_disp(128)\n",
    "\n",
    "        self.upconv3 = upconv(128, 64, 3, 1) #\n",
    "        self.iconv3 = iconv(64 + 64 + 2, 64, 3, 1)\n",
    "        self.disp3_layer = get_disp(64)\n",
    "\n",
    "        self.upconv2 = upconv(64, 32, 3, 2)\n",
    "        self.iconv2 = iconv(64 + 32 + 2, 32, 3, 1)\n",
    "        self.disp2_layer = get_disp(32)\n",
    "\n",
    "        self.upconv1 = upconv(32, 16, 3, 2)\n",
    "        self.iconv1 = iconv(16 + 2, 16, 3, 1)\n",
    "        self.disp1_layer = get_disp(16)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x_conv1 = self.conv1(x)\n",
    "        x_pool1 = self.maxpool(x_conv1)\n",
    "        x1 = self.layer1(x_pool1)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        # skips\n",
    "        skip1 = x_conv1\n",
    "        skip2 = x_pool1\n",
    "        skip3 = x1\n",
    "        skip4 = x2\n",
    "        skip5 = x3\n",
    "        # print(skip4.size())\n",
    "\n",
    "        # decoder\n",
    "        upconv6 = self.upconv6(x4)\n",
    "        concat6 = torch.cat((upconv6, skip5), 1)\n",
    "        iconv6 = self.iconv6(concat6)\n",
    "\n",
    "        upconv5 = self.upconv5(iconv6)\n",
    "        # print(upconv5.size())\n",
    "        concat5 = torch.cat((upconv5, skip4), 1)\n",
    "        iconv5 = self.iconv5(concat5)\n",
    "\n",
    "        upconv4 = self.upconv4(iconv5)\n",
    "        concat4 = torch.cat((upconv4, skip3), 1)\n",
    "        iconv4 = self.iconv4(concat4)\n",
    "        self.disp4 = self.disp4_layer(iconv4)\n",
    "        self.udisp4 = nn.functional.interpolate(self.disp4, scale_factor=1, mode='bilinear', align_corners=True)\n",
    "        self.disp4 = nn.functional.interpolate(self.disp4, scale_factor=0.5, mode='bilinear', align_corners=True)\n",
    "\n",
    "        upconv3 = self.upconv3(iconv4)\n",
    "        concat3 = torch.cat((upconv3, skip2, self.udisp4), 1)\n",
    "        iconv3 = self.iconv3(concat3)\n",
    "        self.disp3 = self.disp3_layer(iconv3)\n",
    "        self.udisp3 = nn.functional.interpolate(self.disp3, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        upconv2 = self.upconv2(iconv3)\n",
    "        concat2 = torch.cat((upconv2, skip1, self.udisp3), 1)\n",
    "        iconv2 = self.iconv2(concat2)\n",
    "        self.disp2 = self.disp2_layer(iconv2)\n",
    "        self.udisp2 = nn.functional.interpolate(self.disp2, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        upconv1 = self.upconv1(iconv2)\n",
    "        concat1 = torch.cat((upconv1, self.udisp2), 1)\n",
    "        iconv1 = self.iconv1(concat1)\n",
    "        self.disp1 = self.disp1_layer(iconv1)\n",
    "        \n",
    "        return self.disp1, self.disp2, self.disp3, self.disp4\n",
    "\n",
    "\n",
    "#ResnetDispModel(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c) Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator:\n",
    "    def __init__(self, val_loader, batch_size, params_file=None, use_gpu=False):\n",
    "        self.use_gpu = use_gpu\n",
    "        self.params_file = params_file\n",
    "        self.val_loader = val_loader\n",
    "        if use_gpu :\n",
    "            self.device = \"cuda:0\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "        self.loss = MonodepthLoss(\n",
    "            n=4,\n",
    "            SSIM_w=0.85,\n",
    "            disp_gradient_w=0.1, lr_w=1).to(self.device)\n",
    "        self.val_losses = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def validate(self, network):\n",
    "\n",
    "        network.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        counter = 0\n",
    "        for i, data in enumerate(self.val_loader):\n",
    "            left, right = data\n",
    "\n",
    "            if self.use_gpu:\n",
    "                left = left.cuda()\n",
    "                network = network.cuda()\n",
    "                right = right.cuda()\n",
    "\n",
    "            model_outputs = network(left)\n",
    "\n",
    "            loss = self.loss(model_outputs, [left, right])\n",
    "            self.val_losses.append(loss.item())\n",
    "            total_loss += loss.item()\n",
    "            counter += 1\n",
    "\n",
    "        total_loss /= self.batch_size * counter\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d) Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, network, train_loader, optimizer, batch_size, params_file=None, use_gpu=False):\n",
    "        self.net = network\n",
    "        self.use_gpu = use_gpu\n",
    "        self.optimizer = optimizer\n",
    "        self.validator = None\n",
    "        self.history = {\"Train\": [], \"Val\": []}\n",
    "        self.params_file = params_file\n",
    "        self.train_loader = train_loader\n",
    "        self.batch_size = batch_size\n",
    "        if use_gpu :\n",
    "            self.device = \"cuda:0\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        self.loss_function = MonodepthLoss(\n",
    "            n=4,\n",
    "            SSIM_w=0.85,\n",
    "            disp_gradient_w=0.1, lr_w=1).to(self.device)\n",
    "\n",
    "\n",
    "    def setValidator(self, validator):\n",
    "        self.validator = validator\n",
    "\n",
    "    def saveParams(self, path):\n",
    "        torch.save(self.net.state_dict(), path)\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        self.net.load_state_dict(torch.load(path))\n",
    "\n",
    "    def train(self):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        self.net.train()\n",
    "        counter = 0\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            left, right = data\n",
    "            if self.use_gpu:\n",
    "                left = left.cuda()\n",
    "                self.net = self.net.cuda()\n",
    "                right = right.cuda()\n",
    "\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            disps = self.net(left)\n",
    "            loss = self.loss_function(disps, [left, right])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            counter += 1\n",
    "\n",
    "        main_loss = total_loss / (counter * self.batch_size)\n",
    "\n",
    "        return main_loss\n",
    "\n",
    "    def run_train(self, epoch):\n",
    "        if self.params_file:\n",
    "            self.loadModel(self.params_file)\n",
    "        prev_score = np.inf\n",
    "        if self.validator:\n",
    "            prev_score = self.validator.validate(self.net)\n",
    "\n",
    "        for e in range(epoch):\n",
    "\n",
    "            loss = self.train()\n",
    "            print(\"Epoch: {} Loss: {}\".format(e, loss))\n",
    "            self.history[\"Train\"].append(loss)\n",
    "\n",
    "            if self.validator:\n",
    "                val_score = self.validator.validate(self.net)\n",
    "                self.history[\"Val\"].append(val_score)\n",
    "                if val_score < prev_score:\n",
    "                    print(\"update model file with prev_score {} and current score {}\".format(prev_score, val_score))\n",
    "                    self.saveParams('params.pkl')\n",
    "                    prev_score = val_score\n",
    "\n",
    "            with open('train_history.pickle', 'wb') as handle:\n",
    "                pickle.dump(self.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def copyNetwork(self):\n",
    "        return copy.deepcopy(self.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5e) Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "\n",
    "val_dataset = TwoViewDataset(\"data/dataset/\", is_train=False, transforms=JointToTensor())\n",
    "trn_dataset = TwoViewDataset(\"data/dataset/\", is_train=True, transforms=JointToTensor())\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=1, num_workers=1, shuffle=False)\n",
    "trn_loader = data.DataLoader(trn_dataset, batch_size=8, num_workers=1, shuffle=False)\n",
    "\n",
    "network = ResnetDispModel(3)\n",
    "val = Validator(val_loader, 1, use_gpu)\n",
    "\n",
    "opt = torch.optim.SGD(network.parameters(), lr=1e-2, weight_decay=1e-6,momentum=0.5, nesterov=False)\n",
    "\n",
    "trn = Trainer(network, trn_loader, opt, 8, use_gpu)\n",
    "\n",
    "trn.setValidator(val)\n",
    "trn.run_train(50)\n",
    "\n",
    "trained_net = trn.net\n",
    "\n",
    "\n",
    "for left, right in val_loader:\n",
    "    sample = left\n",
    "    break\n",
    "\n",
    "sample_np = np.array(sample)\n",
    "sample_np.shape\n",
    "# sample.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disp1, disp2, disp3, disp4 = trained_net(sample.cuda())\n",
    "\n",
    "disp1_np = np.array(disp1.cpu().detach().numpy())\n",
    "disp1_np =np.squeeze(disp1_np)[0]\n",
    "plt.imshow(disp1_np, cmap=\"plasma\")\n",
    "\n",
    "disp2_np = np.array(disp2.cpu().detach().numpy())\n",
    "disp2_np =np.squeeze(disp2_np)[0]\n",
    "plt.imshow(disp2_np, cmap=\"plasma\")\n",
    "\n",
    "disp3_np = np.array(disp3.cpu().detach().numpy())\n",
    "disp3_np =np.squeeze(disp3_np)[0]\n",
    "plt.imshow(disp3_np, cmap=\"plasma\")\n",
    "\n",
    "disp4_np = np.array(disp4.cpu().detach().numpy())\n",
    "disp4_np =np.squeeze(disp4_np)[0]\n",
    "plt.imshow(disp4_np, cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Conclusion section (2-4 paragraphs): summarizing your observations, results, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) References\n",
    "\n",
    "### • Godard, C., Mac Aodha, O., & Brostow, G. (2019). Unsupervised Monocular Depth Estimation with Left-Right             Consistency. Retrieved 13 December 2019, from https://arxiv.org/abs/1609.03677\n",
    "\n",
    "### • OniroAI, MonoDepth-PyTorch, (2018), GitHub repository, https://github.com/OniroAI/MonoDepth-PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
