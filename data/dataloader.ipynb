{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image  # using pillow-simd for increased speed\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as tF\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "        \n",
    "class JointRandomFlip(object):\n",
    "    def __call__(self, L, R):\n",
    "        if np.random.random_sample()>0.5:\n",
    "            return (tF.hflip(R),tF.hflip(L))\n",
    "        return (L,R)\n",
    "    \n",
    "class JointRandomColorAug(object):\n",
    "\n",
    "    def __init__(self,gamma=(0.8,1.2),brightness=(0.5,2.0),color_shift=(0.8,1.2)):\n",
    "        self.gamma = gamma\n",
    "        self.brightness = brightness\n",
    "        self.color_shift = color_shift\n",
    "\n",
    "    def __call__(self, L, R):\n",
    "        if  np.random.random_sample()>0.5:\n",
    "            \n",
    "            random_gamma = np.random.uniform(*self.gamma)\n",
    "            L_aug = L ** random_gamma\n",
    "            R_aug = R ** random_gamma\n",
    "\n",
    "            random_brightness = np.random.uniform(*self.brightness)\n",
    "            L_aug = L_aug * random_brightness\n",
    "            R_aug = R_aug * random_brightness\n",
    "\n",
    "            random_colors = np.random.uniform(self.color_shift[0],self.color_shift[1], 3)\n",
    "            for i in range(3):\n",
    "                L_aug[i, :, :] *= random_colors[i]\n",
    "                R_aug[i, :, :] *= random_colors[i]\n",
    "\n",
    "            # saturate\n",
    "            L_aug = torch.clamp(L_aug, 0, 1)\n",
    "            R_aug = torch.clamp(R_aug, 0, 1)\n",
    "\n",
    "            return L_aug, R_aug\n",
    "\n",
    "        else:\n",
    "            return L, R\n",
    "\n",
    "class JointToTensor(object):\n",
    "    def __call__(self, L, R):\n",
    "        return tF.to_tensor(L),tF.to_tensor(R)\n",
    "    \n",
    "class JointToImage(object):\n",
    "    def __call__(self, L, R):\n",
    "        return transforms.ToPILImage()(L),transforms.ToPILImage()(R)\n",
    "    \n",
    "    \n",
    "class JointCompose(object):\n",
    "    def __init__(self, transforms):\n",
    "        \"\"\"\n",
    "        params: \n",
    "           transforms (list) : list of transforms\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "    # We override the __call__ function such that this class can be\n",
    "    # called as a function i.e. JointCompose(transforms)(img, target)\n",
    "    # Such classes are known as \"functors\"\n",
    "    def __call__(self, img, target):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            img (PIL.Image)    : input image\n",
    "            target (PIL.Image) : ground truth label \n",
    "        \"\"\"\n",
    "        assert img.size == target.size\n",
    "        for t in self.transforms:\n",
    "            img, target = t(img, target)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "train_joint_transform = JointCompose([JointRandomFlip(),JointToTensor(),JointRandomColorAug(),JointToImage()])\n",
    "\n",
    "\n",
    "class TwoViewDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 resize_shape=(1242, 375), \n",
    "                 is_train=False,\n",
    "                 transforms=None,\n",
    "                 sanity_check=None):\n",
    "        super(TwoViewDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.interp = Image.ANTIALIAS\n",
    "        self.resize_shape = resize_shape\n",
    "        self.is_train = is_train\n",
    "        self.transforms=transforms\n",
    "        self.loader = pil_loader\n",
    "        \n",
    "        if is_train:\n",
    "            self.imgR_folder = os.path.join(data_path, \"train\", \"image_right\")\n",
    "            self.imgL_folder = os.path.join(data_path, \"train\", \"image_left\")\n",
    "        else:\n",
    "            self.imgR_folder = os.path.join(data_path, \"val\", \"image_right\")\n",
    "            self.imgL_folder = os.path.join(data_path, \"val\", \"image_left\")\n",
    "        \n",
    "        \n",
    "        self.imgR=[os.path.join(self.imgR_folder, x) for x in os.listdir(self.imgR_folder)]\n",
    "        self.imgL=[os.path.join(self.imgL_folder, x) for x in os.listdir(self.imgL_folder)]\n",
    "\n",
    "    def get_color(self, path, do_flip):\n",
    "        color = self.loader(path)\n",
    "        if do_flip:\n",
    "            color = color.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return self.to_tensor(color)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(list(glob.glob1(self.imgL_folder, \"*.jpg\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        colorR=Image.open(self.imgR[index]).convert('RGB')\n",
    "        colorL=Image.open(self.imgL[index]).convert('RGB')\n",
    "        \n",
    "        print(\"sss\")\n",
    "        if self.transforms is not None:\n",
    "            colorR, colorL = self.transforms(colorR, colorL)\n",
    "        return colorL, colorR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2769\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2770\u001b[1;33m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2771\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-81cc7a234375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoViewDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\f225zhan\\Downloads\\CS684-Project\\data\\dataset'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_joint_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-1cea61a417d9>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mcolorR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[0mcolorL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2770\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2771\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2772\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2773\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "a = TwoViewDataset(r'C:\\Users\\f225zhan\\Downloads\\CS684-Project\\data\\dataset',is_train=True,transforms=train_joint_transform)\n",
    "t=a[:2]\n",
    "print(len(a))\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(2,2,1)\n",
    "ax1.imshow(t[0])\n",
    "ax2 = fig.add_subplot(2,2,2)\n",
    "ax2.imshow(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "v0thTQn_gre1",
    "gMwF-zJXgre2",
    "OgnlA8Uzgre5",
    "NVYUQIYRgrfd",
    "k9-sVkiZgrgV",
    "TwTdBIfzgrgk",
    "zz2PD8vYgrgo",
    "IjfbtREQgrgt",
    "wvxJq5hhgrg-",
    "IIfR_KFfgrhA",
    "YJOMMKqtgrhD",
    "2gKJ-1MRgrhP",
    "hZpDgz-HgriH",
    "Yuq6Z4S7griT",
    "En-mCuOogriW",
    "8Aqvx0y_grib",
    "tf2lwwzWgriw"
   ],
   "name": "MySemanticSegmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
